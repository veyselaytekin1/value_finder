{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ebay webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests    # for making standart html requests\n",
    "from bs4 import BeautifulSoup # magical tool for parsing html data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Altbau': 0, 'Badezimmer': '1', 'Balkon': 1, 'Einbauküche': 1, 'Garage': 0, 'Heizkosten': '0', 'Keller': 0, 'Möbliert': 0, 'Nebenkosten': '420 €', 'Neubau': 0, 'Schlafzimmer': '1', 'Warmmiete': '0', 'Wohnfläche': '68 m²', 'Zimmer': '2', 'kaltmiete': '2.890 €'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class ApartmentScraper:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.soup = None\n",
    "\n",
    "    def fetch_page(self):\n",
    "        \"\"\"Web sayfasini çeker ve BeautifulSoup ile ayristirir.\"\"\"\n",
    "        response = requests.get(self.url)\n",
    "        response.raise_for_status()  # HTTP hata durumlarını kontrol et\n",
    "        self.soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    def get_basic_details(self):\n",
    "        \"\"\"Temel ev bilgilerini çikarir.\"\"\"\n",
    "        valid_keys = {'Wohnfläche', 'Zimmer', 'Schlafzimmer', 'Badezimmer', 'Nebenkosten', 'Heizkosten', 'Warmmiete'}\n",
    "        details_dict = {key: '0' for key in valid_keys}\n",
    "        details = self.soup.find_all(class_='addetailslist--detail')\n",
    "        for detail in details:\n",
    "            detail_value_element = detail.find('span', class_='addetailslist--detail--value')\n",
    "            if detail_value_element:\n",
    "                detail_value_text = detail_value_element.get_text(strip=True)\n",
    "                detail_title_text = detail.get_text(strip=True).replace(detail_value_text, '').strip()\n",
    "                if detail_title_text in valid_keys:\n",
    "                    details_dict[detail_title_text] = detail_value_text\n",
    "        return details_dict\n",
    "\n",
    "    def get_feature_tags(self, details_dict):\n",
    "        \"\"\"Özellik etiketlerini cikarir ve günceller.\"\"\"\n",
    "        valid_features = {'Balkon', 'Einbauküche', 'Garage', 'Keller', 'Möbliert', 'Neubau', 'Altbau'}\n",
    "        features_found = set()\n",
    "        checktag_list = self.soup.find('ul', class_='checktaglist')\n",
    "        if checktag_list:\n",
    "            checktags = checktag_list.find_all('li', class_='checktag')\n",
    "            for checktag in checktags:\n",
    "                feature = checktag.get_text(strip=True)\n",
    "                if feature in valid_features:\n",
    "                    details_dict[feature] = 1\n",
    "                    features_found.add(feature)\n",
    "        \n",
    "        for feature in valid_features:\n",
    "            if feature not in features_found:\n",
    "                details_dict[feature] = 0\n",
    "\n",
    "    def get_kaltmiete(self, details_dict):\n",
    "        \"\"\"Kaltmiete bilgisini cikarir ve günceller.\"\"\"\n",
    "        kaltmiete_element = self.soup.find('h2', class_='boxedarticle--price', id='viewad-price')\n",
    "        if kaltmiete_element:\n",
    "            kaltmiete = kaltmiete_element.get_text(strip=True)\n",
    "            details_dict['kaltmiete'] = kaltmiete\n",
    "        else:\n",
    "            details_dict['kaltmiete'] = '0'  # Eğer kaltmiete bulunamazsa, '0' olarak ayarla\n",
    "\n",
    "    def extract_details(self):\n",
    "        \"\"\"Web sayfasından detaylari cikarir ve bir sözlük olarak döndürür.\"\"\"\n",
    "        if self.soup is None:\n",
    "            raise ValueError(\"Soup nesnesi boş. Önce fetch_page() unu cagirign.\")\n",
    "        \n",
    "        details_dict = self.get_basic_details()\n",
    "        self.get_feature_tags(details_dict)\n",
    "        self.get_kaltmiete(details_dict)\n",
    "        \n",
    "        # Sözlüğü alfabetik olarak sırala ve bir standart dict olarak döndür\n",
    "        return dict(sorted(details_dict.items()))\n",
    "\n",
    "# Kullanımı\n",
    "url = 'https://www.kleinanzeigen.de/s-anzeige/charmantes-voll-moebliertes-2-zimmer-apartment-in-muenchen-lehel-mit-flexibler-mindestmietzeit/2732040224-203-6418'\n",
    "scraper = ApartmentScraper(url)\n",
    "scraper.fetch_page()\n",
    "details = scraper.extract_details()\n",
    "print(details)  # Sonuçları görmek için yazdırabilirsiniz, ancak bu satırı kaldırabilirsiniz.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatgpt ile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detaylar listesi boş. Hiçbir veri çekilemedi.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "def scrape_page(page_url):\n",
    "    response = requests.get(page_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def get_listing_urls(base_url, soup):\n",
    "    listing_urls = []\n",
    "    listings = soup.select('.ad-listitem')\n",
    "    for listing in listings:\n",
    "        relative_url = listing.get('data-href')\n",
    "        if relative_url:\n",
    "            full_url = base_url + relative_url\n",
    "            listing_urls.append(full_url)\n",
    "    return listing_urls\n",
    "\n",
    "def get_listing_details(listing_url):\n",
    "    soup = scrape_page(listing_url)\n",
    "    details = {}\n",
    "    price_tag = soup.find(class_='boxedarticle--price')\n",
    "    if price_tag:\n",
    "        details['price'] = price_tag.text.strip()\n",
    "    features = soup.find_all(class_='addetailslist--detail')\n",
    "    for feature in features:\n",
    "        key = feature.find(class_='addetailslist--detail--label').text.strip()\n",
    "        value = feature.find(class_='addetailslist--detail--value').text.strip()\n",
    "        details[key] = value\n",
    "    return details\n",
    "\n",
    "def scrape_all_pages(base_url, start_page, end_page):\n",
    "    all_details = []\n",
    "    for page_num in range(start_page, end_page + 1):\n",
    "        page_url = f\"{base_url}/seite:{page_num}\"\n",
    "        soup = scrape_page(page_url)\n",
    "        listing_urls = get_listing_urls(base_url, soup)\n",
    "        for url in listing_urls:\n",
    "            details = get_listing_details(url)\n",
    "            all_details.append(details)\n",
    "            print(f\"Detaylar çekildi: {details}\")  # Debugging için eklenen print\n",
    "\n",
    "    if all_details:  # Boş liste kontrolü\n",
    "        with open('rent_listings.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=all_details[0].keys())\n",
    "            writer.writeheader()\n",
    "            for details in all_details:\n",
    "                writer.writerow(details)\n",
    "    else:\n",
    "        print(\"Detaylar listesi boş. Hiçbir veri çekilemedi.\")\n",
    "\n",
    "# Ana URL ve sayfa aralığı ayarları\n",
    "BASE_URL = 'https://www.kleinanzeigen.de'\n",
    "START_PAGE = 1\n",
    "END_PAGE = 5  # Örnek olarak 5 sayfa veri çekilecek\n",
    "\n",
    "scrape_all_pages(BASE_URL, START_PAGE, END_PAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.19 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1707e2a11e748d0db8d27b6ed4dcf0f142b7ddd87c451bd8f4efb955e3e3d619"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
