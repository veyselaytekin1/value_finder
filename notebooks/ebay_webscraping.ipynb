{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ebay webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests    # for making standart html requests\n",
    "from bs4 import BeautifulSoup # magical tool for parsing html data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Wohnfläche': '38 m²', 'Zimmer': '1', 'Verfügbar ab': 'März 2024', 'Online-Besichtigung': 'Möglich', 'Tauschangebot': 'Kein Tausch', 'Nebenkosten': '255 €', 'Warmmiete': '980 €', 'kaltmiete': '725 €', 'Balkon': 0, 'Neubau': 0, 'Möbliert': 0, 'Keller': 0, 'Einbauküche': 0, 'Altbau': 0, 'Garage': 0}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class ApartmentScraper:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.soup = None\n",
    "\n",
    "    def fetch_page(self):\n",
    "        \"\"\"Web sayfasını çeker ve BeautifulSoup ile ayrıştırır.\"\"\"\n",
    "        response = requests.get(self.url)\n",
    "        response.raise_for_status()  # HTTP hata durumlarını kontrol et\n",
    "        self.soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    def extract_details(self):\n",
    "        \"\"\"Web sayfasından detayları çıkarır ve bir sözlük olarak döndürür.\"\"\"\n",
    "        if self.soup is None:\n",
    "            raise ValueError(\"Soup nesnesi boş. Önce fetch_page() metodunu çağırın.\")\n",
    "        \n",
    "        details = self.soup.find_all(class_='addetailslist--detail')\n",
    "        details_dict = {}\n",
    "        for detail in details:\n",
    "            detail_value_element = detail.find('span', class_='addetailslist--detail--value')\n",
    "            if detail_value_element:\n",
    "                detail_value_text = detail_value_element.get_text(strip=True)\n",
    "                detail_title_text = detail.get_text(strip=True).replace(detail_value_text, '').strip()\n",
    "            else:\n",
    "                detail_value_text = 'Değer Yok'\n",
    "                detail_title_text = detail.get_text(strip=True)\n",
    "\n",
    "            details_dict[detail_title_text] = detail_value_text\n",
    "        \n",
    "        # \"boxedarticle--flex--container\" sınıfındaki 'kaltmiete' bilgisini ekle\n",
    "        kaltmiete_element = self.soup.find(class_='boxedarticle--flex--container')\n",
    "        if kaltmiete_element:\n",
    "            kaltmiete_value = kaltmiete_element.get_text(strip=True)\n",
    "            details_dict['kaltmiete'] = kaltmiete_value\n",
    "        else:\n",
    "            details_dict['kaltmiete'] = 'Bilgi Bulunamadı'\n",
    "\n",
    "        # \"checktaglist\" sınıfından özellikleri çıkar ve uygun şekilde işaretle\n",
    "        valid_features = {'Balkon', 'Einbauküche', 'Garage', 'Keller', 'Möbliert', 'Neubau', 'Altbau'}\n",
    "        features_found = set()\n",
    "        checktag_list = self.soup.find('ul', class_='checktaglist')\n",
    "        if checktag_list:\n",
    "            checktags = checktag_list.find_all('li', class_='checktag')\n",
    "            for checktag in checktags:\n",
    "                feature = checktag.get_text(strip=True)\n",
    "                if feature in valid_features:\n",
    "                    details_dict[feature] = 1\n",
    "                    features_found.add(feature)\n",
    "        \n",
    "        # valid_features içinde ve sayfada bulunmayan özellikler için '0' değeri ekle\n",
    "        for feature in valid_features:\n",
    "            if feature not in features_found:\n",
    "                details_dict[feature] = 0\n",
    "\n",
    "        return details_dict\n",
    "\n",
    "# Kullanımı\n",
    "url = 'https://www.kleinanzeigen.de/s-anzeige/erstbezug-ab-sofort-donau-side-modernes-studenten-apartment-mit-fitnessstudio-co-working/2732860091-203-7613'\n",
    "scraper = ApartmentScraper(url)\n",
    "scraper.fetch_page()\n",
    "details = scraper.extract_details()\n",
    "print(details)  # Sonuçları görmek için yazdırabilirsiniz, ancak bu satırı kaldırabilirsiniz.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Altbau': 0, 'Badezimmer': '1', 'Balkon': 0, 'Einbauküche': 1, 'Garage': 0, 'Heizkosten': '50 €', 'Keller': 1, 'Möbliert': 0, 'Nebenkosten': '100 €', 'Neubau': 1, 'Schlafzimmer': '1', 'Warmmiete': '1.500 €', 'Wohnfläche': '40 m²', 'Zimmer': '2'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class ApartmentScraper:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.soup = None\n",
    "\n",
    "    def fetch_page(self):\n",
    "        \"\"\"Web sayfasını çeker ve BeautifulSoup ile ayrıştırır.\"\"\"\n",
    "        response = requests.get(self.url)\n",
    "        response.raise_for_status()  # HTTP hata durumlarını kontrol et\n",
    "        self.soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    def extract_details(self):\n",
    "        \"\"\"Web sayfasından detayları çıkarır ve bir sözlük olarak döndürür.\"\"\"\n",
    "        if self.soup is None:\n",
    "            raise ValueError(\"Soup nesnesi boş. Önce fetch_page() metodunu çağırın.\")\n",
    "        \n",
    "        valid_keys = {'Wohnfläche', 'Zimmer', 'Schlafzimmer', 'Badezimmer', 'Nebenkosten', 'Heizkosten', 'Warmmiete'}\n",
    "        details_dict = {key: '0' for key in valid_keys}  # Önceden belirlenen anahtarlar için sözlük oluştur ve değerleri '0' olarak ayarla\n",
    "        \n",
    "        details = self.soup.find_all(class_='addetailslist--detail')\n",
    "        for detail in details:\n",
    "            detail_value_element = detail.find('span', class_='addetailslist--detail--value')\n",
    "            if detail_value_element:\n",
    "                detail_value_text = detail_value_element.get_text(strip=True)\n",
    "                detail_title_text = detail.get_text(strip=True).replace(detail_value_text, '').strip()\n",
    "                if detail_title_text in valid_keys:\n",
    "                    details_dict[detail_title_text] = detail_value_text\n",
    "        \n",
    "        # \"checktaglist\" sınıfından özellikleri çıkar ve uygun şekilde işaretle\n",
    "        valid_features = {'Balkon', 'Einbauküche', 'Garage', 'Keller', 'Möbliert', 'Neubau', 'Altbau'}\n",
    "        features_found = set()\n",
    "        checktag_list = self.soup.find('ul', class_='checktaglist')\n",
    "        if checktag_list:\n",
    "            checktags = checktag_list.find_all('li', class_='checktag')\n",
    "            for checktag in checktags:\n",
    "                feature = checktag.get_text(strip=True)\n",
    "                if feature in valid_features:\n",
    "                    details_dict[feature] = 1\n",
    "                    features_found.add(feature)\n",
    "        \n",
    "        # valid_features içinde ve sayfada bulunmayan özellikler için '0' değeri ekle\n",
    "        for feature in valid_features:\n",
    "            if feature not in features_found:\n",
    "                details_dict[feature] = 0\n",
    "\n",
    "        # Sözlüğü alfabetik olarak sırala ve bir standart dict olarak döndür\n",
    "        return dict(sorted(details_dict.items()))\n",
    "\n",
    "# Kullanımı\n",
    "url = 'https://www.kleinanzeigen.de/s-anzeige/zentrumsnahe-2-zimmer-wohnung-muenchen-neuhausen-/2756014016-203-6424'\n",
    "scraper = ApartmentScraper(url)\n",
    "scraper.fetch_page()\n",
    "details = scraper.extract_details()\n",
    "print(details)  # Sonuçları görmek için yazdırabilirsiniz, ancak bu satırı kaldırabilirsiniz.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Altbau': 0, 'Badezimmer': '0', 'Balkon': 0, 'Einbauküche': 0, 'Garage': 0, 'Heizkosten': '0', 'Keller': 0, 'Möbliert': 0, 'Nebenkosten': '255 €', 'Neubau': 0, 'Schlafzimmer': '0', 'Warmmiete': '980 €', 'Wohnfläche': '38 m²', 'Zimmer': '1'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class ApartmentScraper:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.soup = None\n",
    "\n",
    "    def fetch_page(self):\n",
    "        \"\"\"Web sayfasını çeker ve BeautifulSoup ile ayrıştırır.\"\"\"\n",
    "        response = requests.get(self.url)\n",
    "        response.raise_for_status()  # HTTP hata durumlarını kontrol et\n",
    "        self.soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    def get_basic_details(self):\n",
    "        \"\"\"Temel ev bilgilerini çıkarır.\"\"\"\n",
    "        valid_keys = {'Wohnfläche', 'Zimmer', 'Schlafzimmer', 'Badezimmer', 'Nebenkosten', 'Heizkosten', 'Warmmiete'}\n",
    "        details_dict = {key: '0' for key in valid_keys}\n",
    "        details = self.soup.find_all(class_='addetailslist--detail')\n",
    "        for detail in details:\n",
    "            detail_value_element = detail.find('span', class_='addetailslist--detail--value')\n",
    "            if detail_value_element:\n",
    "                detail_value_text = detail_value_element.get_text(strip=True)\n",
    "                detail_title_text = detail.get_text(strip=True).replace(detail_value_text, '').strip()\n",
    "                if detail_title_text in valid_keys:\n",
    "                    details_dict[detail_title_text] = detail_value_text\n",
    "        return details_dict\n",
    "\n",
    "    def get_feature_tags(self, details_dict):\n",
    "        \"\"\"Özellik etiketlerini çıkarır ve günceller.\"\"\"\n",
    "        valid_features = {'Balkon', 'Einbauküche', 'Garage', 'Keller', 'Möbliert', 'Neubau', 'Altbau'}\n",
    "        features_found = set()\n",
    "        checktag_list = self.soup.find('ul', class_='checktaglist')\n",
    "        if checktag_list:\n",
    "            checktags = checktag_list.find_all('li', class_='checktag')\n",
    "            for checktag in checktags:\n",
    "                feature = checktag.get_text(strip=True)\n",
    "                if feature in valid_features:\n",
    "                    details_dict[feature] = 1\n",
    "                    features_found.add(feature)\n",
    "        \n",
    "        for feature in valid_features:\n",
    "            if feature not in features_found:\n",
    "                details_dict[feature] = 0\n",
    "\n",
    "    def extract_details(self):\n",
    "        \"\"\"Web sayfasından detayları çıkarır ve bir sözlük olarak döndürür.\"\"\"\n",
    "        if self.soup is None:\n",
    "            raise ValueError(\"Soup nesnesi boş. Önce fetch_page() metodunu çağırın.\")\n",
    "        \n",
    "        details_dict = self.get_basic_details()\n",
    "        self.get_feature_tags(details_dict)\n",
    "        \n",
    "        # Sözlüğü alfabetik olarak sırala ve bir standart dict olarak döndür\n",
    "        return dict(sorted(details_dict.items()))\n",
    "\n",
    "# Kullanımı\n",
    "url = 'https://www.kleinanzeigen.de/s-anzeige/erstbezug-ab-sofort-donau-side-modernes-studenten-apartment-mit-fitnessstudio-co-working/2732860091-203-7613'\n",
    "scraper = ApartmentScraper(url)\n",
    "scraper.fetch_page()\n",
    "details = scraper.extract_details()\n",
    "print(details)  # Sonuçları görmek için yazdırabilirsiniz, ancak bu satırı kaldırabilirsiniz.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatgpt ile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detaylar listesi boş. Hiçbir veri çekilemedi.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "def scrape_page(page_url):\n",
    "    response = requests.get(page_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def get_listing_urls(base_url, soup):\n",
    "    listing_urls = []\n",
    "    listings = soup.select('.ad-listitem')\n",
    "    for listing in listings:\n",
    "        relative_url = listing.get('data-href')\n",
    "        if relative_url:\n",
    "            full_url = base_url + relative_url\n",
    "            listing_urls.append(full_url)\n",
    "    return listing_urls\n",
    "\n",
    "def get_listing_details(listing_url):\n",
    "    soup = scrape_page(listing_url)\n",
    "    details = {}\n",
    "    price_tag = soup.find(class_='boxedarticle--price')\n",
    "    if price_tag:\n",
    "        details['price'] = price_tag.text.strip()\n",
    "    features = soup.find_all(class_='addetailslist--detail')\n",
    "    for feature in features:\n",
    "        key = feature.find(class_='addetailslist--detail--label').text.strip()\n",
    "        value = feature.find(class_='addetailslist--detail--value').text.strip()\n",
    "        details[key] = value\n",
    "    return details\n",
    "\n",
    "def scrape_all_pages(base_url, start_page, end_page):\n",
    "    all_details = []\n",
    "    for page_num in range(start_page, end_page + 1):\n",
    "        page_url = f\"{base_url}/seite:{page_num}\"\n",
    "        soup = scrape_page(page_url)\n",
    "        listing_urls = get_listing_urls(base_url, soup)\n",
    "        for url in listing_urls:\n",
    "            details = get_listing_details(url)\n",
    "            all_details.append(details)\n",
    "            print(f\"Detaylar çekildi: {details}\")  # Debugging için eklenen print\n",
    "\n",
    "    if all_details:  # Boş liste kontrolü\n",
    "        with open('rent_listings.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=all_details[0].keys())\n",
    "            writer.writeheader()\n",
    "            for details in all_details:\n",
    "                writer.writerow(details)\n",
    "    else:\n",
    "        print(\"Detaylar listesi boş. Hiçbir veri çekilemedi.\")\n",
    "\n",
    "# Ana URL ve sayfa aralığı ayarları\n",
    "BASE_URL = 'https://www.kleinanzeigen.de'\n",
    "START_PAGE = 1\n",
    "END_PAGE = 5  # Örnek olarak 5 sayfa veri çekilecek\n",
    "\n",
    "scrape_all_pages(BASE_URL, START_PAGE, END_PAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.19 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1707e2a11e748d0db8d27b6ed4dcf0f142b7ddd87c451bd8f4efb955e3e3d619"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
